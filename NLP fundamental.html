<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>NLP concepts for human processing</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="fe898e63-9eb9-462c-92cd-f97a1d48e264" class="page sans"><header><img class="page-cover-image" src="NLP%20concepts%20for%20human%20processing%20fe898e639eb9462c92cdf97a1d48e264/NLP.jpeg" style="object-position:center 50%"/><h1 class="page-title"><strong><strong>NLP concepts for human processing</strong></strong></h1><p class="page-description"></p><table class="properties"><tbody></tbody></table></header><div class="page-body"><p id="54a07ce0-d883-4a08-a77d-26e31b063ade" class="">
</p><p id="4a665048-9883-4555-86c3-25a4e459fc50" class="">Natural language processing (NLP) concepts that humans can read and understand.</p><p id="4eaa7113-09f4-49d5-8667-c4fd9eda1874" class="">
</p><p id="a2203415-cec3-442c-9f8c-896668e36275" class="">Why it matters — There is a lot of buzz around GPT (OpenAI model) and BERT (google AI model). I believe AI is not a FAD. Learning the concept behind it prepares you to utilize this technology for your needs better.</p><p id="a062948e-6e70-4f6d-b246-9576f5a0ba7f" class="">
</p><p id="b620f1be-3887-4975-88f6-8ed667fa2aba" class="">Key Focus — Word Embedding and Transformers</p><p id="68e33a80-b087-4033-8f7b-3ab95d1c0f57" class="">
</p><p id="4e6b975c-626d-464a-aed8-dad5cbabf518" class="">I enjoy simplicity &amp; you can learn anything by understanding at the basic concept level.</p><p id="ee245e7b-246a-40b7-816f-7ef22bd86254" class="">First thing first, my benefits — what can I use it for?</p><h2 id="96ff859a-af89-4f65-9452-149c48eb96b8" class=""><strong>Use-cases</strong></h2><ol type="1" id="84619acf-c50e-49d2-982a-d19b424a70c1" class="numbered-list" start="1"><li><strong>Translation</strong>: Language translator such as English → Polish</li></ol><ol type="1" id="f8da080c-ed10-4ec5-9c49-cb4bae1b9f0b" class="numbered-list" start="2"><li><strong>Text classification</strong>: Categorization of input text, such as sentiment analysis and new idea vs. feature improvement requests tags from customer feedback.</li></ol><ol type="1" id="29a1e626-c22a-4761-a60c-ae4bca1c353c" class="numbered-list" start="3"><li><strong>Semantic analysis</strong>: Understanding the meaning and context of the input</li></ol><ol type="1" id="71e8c5ba-e049-46bd-b8d7-dbafbdff7540" class="numbered-list" start="4"><li><strong>Part-of-speech tagging</strong>: Tagging a particular part of speech, depending on the definition of the word and its context. Ex- the word “Book” has different tagging in the “Book the flight” and “Reading a book” phrases.</li></ol><ol type="1" id="9a8b8000-f5c5-4abf-bc60-def72f1a27af" class="numbered-list" start="5"><li><strong>Text summarization</strong>: summary or insight from the input text block or document</li></ol><ol type="1" id="5eb70b9c-8026-4a95-ad51-d4873f6eac4e" class="numbered-list" start="6"><li><strong>Generative models</strong>: create something new using available data, such as creating a photo or creating poetry from a text description.</li></ol><p id="aa1622f8-c063-4071-94db-5b08cfd13e3c" class="">This explanation is just a spark; let your creativity fire spread and blow it out of proportion.</p><h2 id="5454f260-6baf-4906-af99-a9b393966de9" class=""><strong>Three key components of NLP:</strong></h2><ol type="1" id="3d5935af-d402-440d-af91-677ecddaaf82" class="numbered-list" start="1"><li><strong>Tokenizing</strong> — create a word or sentence block elements (Token) from an input</li></ol><ol type="1" id="06a4307e-f418-4afd-9562-7b604b0a6857" class="numbered-list" start="2"><li><strong>Word Normalization</strong> — a mathematical vector representation of tokens. Think of this as a translation of your input into the input that the computer can understand.1. Stemming: cutting words. (e.g.: Signer, Signing → Sign)2. Lemmatization: Finding root words. (e.g.: Sung, Singing → Sing)3. <strong>Word Embedding</strong>: converting words into a vector of real numbers. <em>(P.s majority of complexity lies here)</em></li></ol><ol type="1" id="37412d0e-41c8-402e-82c0-16f388a7b88c" class="numbered-list" start="3"><li><strong>Functional Models</strong> — Run processing model on normalized output for particular functional use cases such as language translation, text summarization, etc.</li></ol><h2 id="ed97eb6c-1aee-47f6-a705-6c2adfd4bdae" class=""><strong>Why is embedding the most important &amp; complex part?</strong></h2><p id="966f6469-ffb0-4ae3-9db8-29a23733fb0a" class="">You can have fixed embedding, meaning a fixed mathematical vector representation of a word or a character. But the storage &amp; time is the essence of any process. You want to optimize the storage and time of your process by building a vector representation of your word or a character that is relevant to the context of that word and your use case.</p><p id="ddd47240-f2bb-46cf-8c7b-b3feedc776e2" class="">For example: “I didn’t like the T-shirt. Its blue color gives an old look”</p><p id="30555a54-285b-47d6-8366-aefaed55e8e4" class="">Fixed embedding of the “Blue” word for language translation vs. keyword extraction use case is costly. You can optimize it based on the context and your use case.</p><h2 id="b2db9321-27ea-45c7-ae50-97770c9577b9" class=""><strong>Embedding models concepts:</strong></h2><ol type="1" id="ae8d36d2-15ed-4908-a893-53f5f2376f64" class="numbered-list" start="1"><li>RNN — recurrent neural network</li></ol><ol type="1" id="7ed70b41-ec99-40d8-b5d6-ad25e6e46f70" class="numbered-list" start="2"><li>CNN — Convolution neural network</li></ol><ol type="1" id="e049d654-a9a8-4a5a-9938-34f9a75f750f" class="numbered-list" start="3"><li>Transformers</li></ol><h2 id="d4ee4138-e8c8-413f-a0b1-76bc1a0637e8" class=""><strong>RNN</strong></h2><p id="d62193b1-38f2-4905-a987-54681c11e52e" class="">RNN is an autoregressive model meaning the processing of a previous word is necessary for the process of the next word. Similar to how our brain process the words one word after another. Remember your SAT/GRE/GMAT English test? How hard was it to understand complex long sentence. That would be the same issue RNN models will face for processing long sequences because of previous word dependency. RNN is a sequential model, and parallelization is restricted.</p><h2 id="45e7a0b1-0224-4b36-82a1-5e7e2dafca57" class=""><strong>CNN</strong></h2><p id="aafd39b5-744c-4029-adef-c2a7232c78d0" class="">CNN takes the meaning of a word by aggregating the local information from its neighboring words. Convolution operation means re-estimating the word&#x27;s weighted average based on the surrounding words. CNN builds a hierarchical representation of words to derive the composition of the input sequence structure. Since there is no dependency on sequential processes, CNN allows parallelization. Hence embedding layer training is faster than RNN, but because of hierarchical representation, it consumes a lot of memory. That means processing long sequences is also problematic for CNN models.</p><h2 id="4721db96-b3e2-4294-a13b-98ae4be18ac3" class=""><strong>Transformer</strong></h2><p id="d6b0f457-9890-4cc5-b9ec-36b9b4467910" class="">Transformer introduced a new modeling paradigm. In contrast to previous models where processing within the encoder and decoder was done with recurrence or convolutions, the transformer operates using only attention. More specifically, it utilizes an extension of attention called self-attention. It is a de-facto leader in NLP nowadays. OpenAPI GPT and Google BERT are types of transformers. These advanced models utilize an extension of the self-attention mechanism called multi-head attention.</p><h2 id="ad1044bb-4d4f-4e22-9322-94e351969c34" class=""><strong>What is Attention?</strong></h2><p id="4b43aab3-8ea2-4df2-9d33-345bb372167c" class="">Sentence is made of objects, details of objects, and their relationship.</p><p id="b26c1994-26aa-40b5-9157-87e45216a90a" class="">Ex: “Shrijit loves a blue color car.” Shrijit &amp; car are the objects. Blue color is a detail of the car object, and love is the relationship between those objects.</p><p id="7b21f101-516f-4692-9c76-f34f2d492071" class="">Details of objects and their relationship is called the <strong>intermediate state</strong>. Without this intermediate state whole context of this sentence is worthless. So the proper representation of the intermediate state is very important.</p><p id="5dd1b049-0d91-4f52-80d7-320fec963fe3" class="">The size of the intermediate state vector is fixed in RNN &amp; CNN models.</p><p id="8fbb5a68-d98c-43b7-ae66-82ebc61596bc" class="">For a very long sentence, the intermediate state becomes very large. Now to improve performance and avoid a single point of failure, the intermediate state information can be spread throughout the sequence of annotation. That is called hidden states.</p><p id="7b093669-3478-450b-9073-b6fdb0a3dbc0" class="">The attention model is having an access to all the hidden states rather than one blotted intermediate state.</p><p id="07ade87e-b010-4f87-b6bf-7c6df012ad98" class=""><strong>Self-attention is </strong>relating different positions of a single sequence in order to compute a representation of the sequence.</p><h2 id="2a6911e8-579a-4102-82c3-6a4c280941ac" class=""><strong>Training an embedding layer:</strong></h2><ol type="1" id="9b3f06b8-c2ca-4c92-bd66-998500da7b3d" class="numbered-list" start="1"><li>Build from Scratch <em>[why kill yourself!]</em></li></ol><ol type="1" id="12044ed4-2083-4f5a-88f7-68297b2969af" class="numbered-list" start="2"><li>Use pre-trained models (such as — GloVe, ELMo, Word2vec, transformers, etc.) <em>[Best for most]</em></li></ol><ol type="1" id="ab1d082b-cd48-47b9-aa1a-881e73957c1c" class="numbered-list" start="3"><li>Hybrid — start with a pre-trained model and fine-tune it based on the need <em>[Best for enterprises]</em></li></ol><p id="496f59ac-e02f-4292-a3ac-70ea7c8aa9ea" class="">
</p><p id="7b2eb611-845f-4022-9a96-815295b06db9" class="">I’ll be writing NLP use case articles with implementation code. If that interest you, stay tuned! <a href="https://medium.com/@patel_shrijit">Medium</a> or <a href="https://shrijitpatel.substack.com/">Substack</a></p><p id="721fc9cd-3568-4e85-816f-11f159fe146e" class="">
</p><p id="d636d05e-c760-46e9-8c84-a3c3dcd929a7" class="">If you enjoy what you read or have suggestions, hit me up on <a href="https://twitter.com/jstAHomosapien">Twitter</a> or <a href="https://www.linkedin.com/in/shrijit-patel/">Linkedin</a>.</p><p id="1b8305b0-9946-4368-99e4-2aabc8a70703" class="">Wanna go deep? Let’s dive!</p><p id="73f6120a-8265-4aea-86ee-4948d8f99aeb" class="">
</p><p id="cfe41ef6-f00f-4f99-9a8c-a01608bf1544" class="">[Amazing Article] <a href="https://medium.com/@RobinVetsch/nlp-from-word-embedding-to-transformers-76ae124e6281">https://medium.com/@RobinVetsch/nlp-from-word-embedding-to-transformers-76ae124e6281</a>.</p><p id="39fce881-1a78-4666-bf84-69dd64685def" class=""><a href="https://towardsdatascience.com/transformers-89034557de14">https://towardsdatascience.com/transformers-89034557de14</a>.</p><p id="2afefd41-5505-4a38-94eb-f5dec110697a" class=""><a href="https://towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e">https://towardsdatascience.com/attaining-attention-in-deep-learning-a712f93bdb1e</a>.</p><p id="46726e24-7540-4435-b1f8-5eceedaa17d6" class=""><a href="https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b">https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b</a>.</p><p id="bd07912c-869f-4d33-b155-4dedb1226418" class="">
</p><p id="3227b7af-9c6a-4426-8e80-d53290093245" class="">You do you! Peace ✌️</p><p id="adac4ada-c235-46b9-a7b9-29667a61a202" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>